{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Импорт необходимых библиотек","metadata":{"execution":{"iopub.status.busy":"2022-09-05T08:37:50.640726Z","iopub.execute_input":"2022-09-05T08:37:50.641071Z","iopub.status.idle":"2022-09-05T08:37:50.650949Z","shell.execute_reply.started":"2022-09-05T08:37:50.641019Z","shell.execute_reply":"2022-09-05T08:37:50.650071Z"}}},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, random_split, DataLoader\nfrom PIL import Image\nimport torchvision.models as models\nfrom tqdm.notebook import tqdm\nimport torchvision.transforms as T\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchvision.utils import make_grid\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensor\nfrom sklearn.model_selection import train_test_split\nimport os\nimport re\nimport requests\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport random\nfrom sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:23.488075Z","iopub.execute_input":"2022-09-11T17:44:23.488367Z","iopub.status.idle":"2022-09-11T17:44:26.678326Z","shell.execute_reply.started":"2022-09-11T17:44:23.488338Z","shell.execute_reply":"2022-09-11T17:44:26.677523Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Список классов заболеваний + лейблы датасета","metadata":{}},{"cell_type":"code","source":"classes = [\"Nutrient surplus\", \"Magnesium\", \"phosphate\", \"Healthy\", \"Phosphorous\", \"nitrates\", \"potassium\", \"nitrogen\", \"calcium\", \"Sulfur\"]\nclasses_labels = {\n        \"Nutrient surplus\": [0],\n        \"Magnesium+phosphate\": [1, 2],\n        \"Healthy\": [3],\n        \"Phosphorous+magnesium\": [4, 1],\n        \"nitrates+potassium\": [5, 6],\n        \"nitrogen+potassium\": [7, 6],\n        \"calcium+phosporous\": [8, 4],\n        \"Sulfur+magnesium\": [9, 1],\n    }\nprint(len(classes_labels), len(classes))","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:26.680026Z","iopub.execute_input":"2022-09-11T17:44:26.680367Z","iopub.status.idle":"2022-09-11T17:44:26.688203Z","shell.execute_reply.started":"2022-09-11T17:44:26.680332Z","shell.execute_reply":"2022-09-11T17:44:26.687313Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Некоторые вспомогательные функции\n* `seed_everything` - функция закрепляющая seed для дальнейшей воспроизводимости экспериментов\n* `get_path_names` - функция для парсинга папок датасета\n* `encode_label` - функция энкодинга лейблов датасета\n* `decode_target` - функция декодинга ответа модели по трешхолду\n* `denorm` - функция денормализации тензоров\n* `show_example` - функция для отображения элемента датасета \n* `show_batch` - функция для отображения батча изображений","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n# Making a list that contains the paths of each image\ndef get_path_names(dir):\n    images = []\n    for path, subdirs, files in os.walk(data_dir):\n        for sub in subdirs:\n            data_path = os.path.join(path, sub)\n            import pdb;pdb.set_trace()\n            for p, s, f in os.walk(data_path):\n                for n in f:\n                    images.append(os.path.join(p, n))\n    return images\n\ndef encode_label(label, classes_list = classes): #encoding the classes into a tensor of shape (10) with 0 and 1s.\n    target = torch.zeros(10)\n    for l in eval(label):\n        target[l] = 1\n    return target\n\n\ndef decode_target(target, threshold=0.5): #decoding the prediction tensors of 0s and 1s into text form\n    result = []\n    for i, x in enumerate(target):\n        if (x >= threshold):\n            result.append(classes[i])     \n    return ' '.join(result)\n\n\ndef show_example(img,label):\n    plt.imshow(img.permute(1, 2, 0))\n    print(\"Label:\", decode_target(label))\n    print()\n    print(label)\n    \n#let's see a batch of images (16 images) in a grid\ndef show_batch(dl, nmax=16):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(8, 8))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images[:nmax], nrow=4).permute(1, 2, 0))\n        break\n\nseed_everything(42)","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:26.689564Z","iopub.execute_input":"2022-09-11T17:44:26.690147Z","iopub.status.idle":"2022-09-11T17:44:26.708545Z","shell.execute_reply.started":"2022-09-11T17:44:26.690090Z","shell.execute_reply":"2022-09-11T17:44:26.707605Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data_dir = \"../input/gidtowerdatasetmultilabel/final_cropped_dataset/final_cropped_dataset\"\nprint(os.listdir(data_dir))\nprint(len(os.listdir(data_dir)))","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:26.710269Z","iopub.execute_input":"2022-09-11T17:44:26.710557Z","iopub.status.idle":"2022-09-11T17:44:26.728039Z","shell.execute_reply.started":"2022-09-11T17:44:26.710532Z","shell.execute_reply":"2022-09-11T17:44:26.727383Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Считывание и преобразование csv файла датасета","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/gidtowerdatasetmultilabel/dataset.csv/dataset.csv\")\ndf = df[[\"imgs\", \"labels\"]]\ndf","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:26.731980Z","iopub.execute_input":"2022-09-11T17:44:26.732247Z","iopub.status.idle":"2022-09-11T17:44:26.763644Z","shell.execute_reply.started":"2022-09-11T17:44:26.732222Z","shell.execute_reply":"2022-09-11T17:44:26.762807Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"#### В датасете есть сломанное изображение - берем все значения кроме сломанного","metadata":{}},{"cell_type":"code","source":"df = df[df[\"imgs\"]!=\"final_cropped_dataset/Sulfur+magnesium/69.jpg\"]\ndf = df[df[\"imgs\"] != \"final_cropped_dataset/Healthy/.DS_Store\"]\ndf = df[df[\"imgs\"] != \"final_cropped_dataset/Nutrient surplus/.DS_Store\"]\ndf = df[[\"imgs\", \"labels\"]]\ndf = df.reset_index()\ndf","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:26.766294Z","iopub.execute_input":"2022-09-11T17:44:26.766639Z","iopub.status.idle":"2022-09-11T17:44:26.786147Z","shell.execute_reply.started":"2022-09-11T17:44:26.766605Z","shell.execute_reply":"2022-09-11T17:44:26.785323Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_df, test_df = train_test_split(df, random_state=42, stratify=df.labels, test_size=0.15)\ntrain_df = train_df.reset_index()\ntest_df = test_df.reset_index()\ntest_df = test_df[[\"imgs\", \"labels\"]]\ntrain_df = train_df[[\"imgs\", \"labels\"]]","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:26.787608Z","iopub.execute_input":"2022-09-11T17:44:26.788176Z","iopub.status.idle":"2022-09-11T17:44:26.800873Z","shell.execute_reply.started":"2022-09-11T17:44:26.788140Z","shell.execute_reply":"2022-09-11T17:44:26.799853Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:26.802037Z","iopub.execute_input":"2022-09-11T17:44:26.802594Z","iopub.status.idle":"2022-09-11T17:44:26.814011Z","shell.execute_reply.started":"2022-09-11T17:44:26.802559Z","shell.execute_reply":"2022-09-11T17:44:26.813004Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Кастомный класс датасета","metadata":{}},{"cell_type":"code","source":"# A class to create a Custom Dataset that will load images and encode the labels of those images from their folder names\nclass myDataset(Dataset):\n    def __init__(self, df, root_dir, transform=None):\n        self.transform = transform\n        self.df = df\n        self.root_dir = root_dir\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_path = self.root_dir + self.df[\"imgs\"][idx]\n        img = np.asarray(Image.open(img_path).convert(\"RGB\"))\n        labels = self.df[\"labels\"][idx]\n        if self.transform:\n            img = self.transform(image=img)[\"image\"]\n        return torch.Tensor(img).permute(2, 1, 0), encode_label(labels)","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:26.815315Z","iopub.execute_input":"2022-09-11T17:44:26.815883Z","iopub.status.idle":"2022-09-11T17:44:26.824634Z","shell.execute_reply.started":"2022-09-11T17:44:26.815848Z","shell.execute_reply":"2022-09-11T17:44:26.823841Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Аугментации, которые мы применим к изображениям датасета","metadata":{}},{"cell_type":"code","source":"imagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # mean and std values of the Imagenet Dataset so that pretrained models could also be used\n\ntrain_transform = A.Compose([A.Resize(224, 224),\n                             A.ShiftScaleRotate(0.2, 0.2, 180),\n                      A.HorizontalFlip(0.5),\n                      A.VerticalFlip(0.5),\n                      A.HueSaturationValue(p=0.5, hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2),\n#                       ToTensor(),\n                      A.Normalize()])\n\nval_transform = A.Compose([A.Resize(224, 224),\n                      A.Normalize()])","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:26.826875Z","iopub.execute_input":"2022-09-11T17:44:26.827471Z","iopub.status.idle":"2022-09-11T17:44:26.835139Z","shell.execute_reply.started":"2022-09-11T17:44:26.827437Z","shell.execute_reply":"2022-09-11T17:44:26.834141Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Creating a dataset that loads images from the specified directory, encode their labels and transforming them into tensors.\nroot_dir = \"../input/gidtowerdatasetmultilabel/final_cropped_dataset/\"\n\ntrain_dataset = myDataset(train_df, root_dir=root_dir, transform = train_transform)\nval_dataset = myDataset(test_df, root_dir=root_dir, transform = val_transform)\nlen(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:26.837196Z","iopub.execute_input":"2022-09-11T17:44:26.837827Z","iopub.status.idle":"2022-09-11T17:44:26.847529Z","shell.execute_reply.started":"2022-09-11T17:44:26.837791Z","shell.execute_reply":"2022-09-11T17:44:26.846586Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"show_example(*val_dataset[12]) #let's take an example","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:26.849134Z","iopub.execute_input":"2022-09-11T17:44:26.849727Z","iopub.status.idle":"2022-09-11T17:44:27.183847Z","shell.execute_reply.started":"2022-09-11T17:44:26.849692Z","shell.execute_reply":"2022-09-11T17:44:27.182932Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Создание Dataloader для использования batch-ей данных при обучении","metadata":{}},{"cell_type":"code","source":"#setting batch size for Dataloader to load the data batch by batch\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size * 2)","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:27.185467Z","iopub.execute_input":"2022-09-11T17:44:27.186051Z","iopub.status.idle":"2022-09-11T17:44:27.191533Z","shell.execute_reply.started":"2022-09-11T17:44:27.186010Z","shell.execute_reply":"2022-09-11T17:44:27.190611Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Отображение батча ","metadata":{}},{"cell_type":"code","source":"show_batch(val_loader)","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:27.193305Z","iopub.execute_input":"2022-09-11T17:44:27.193637Z","iopub.status.idle":"2022-09-11T17:44:29.905955Z","shell.execute_reply.started":"2022-09-11T17:44:27.193603Z","shell.execute_reply":"2022-09-11T17:44:29.905124Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Класс с основными методами для Multilabel моделей","metadata":{}},{"cell_type":"code","source":"class MultilabelImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        \"\"\"Get from training batch image and target, then push to model,\n        finaly calculate loss\n        \"\"\"\n        \n        images, targets = batch \n        out = self(images)                            # Generate predictions\n        loss = F.binary_cross_entropy(out, targets)   # Calculate loss\n        return loss    \n\n    def validation_step(self, batch):\n        \"\"\"Get from val batch image and target, then push to model,\n        compute loss and output arrays for metric count\n        \"\"\"\n        \n        pred_Y = []\n        true_Y = []\n        images, targets = batch \n        out = self(images)                           # Generate predictions\n        loss = F.binary_cross_entropy(out, targets)  # Calculate loss\n        pred_Y.append(np.array((out.detach().to(\"cpu\") > 0.5).float()))\n        true_Y.append(np.array(targets.detach().to(\"cpu\").tolist()))\n        return {'val_loss': loss.detach(), \"metrics\": (np.array(pred_Y), np.array(true_Y))}      \n\n\n    def validation_epoch_end(self, outputs):\n        \"\"\"Count scores (losses, classification metrics) at the end of val epoch\"\"\"\n        \n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()      # Combine losses and get the mean value\n        batch_scores = [x['metrics'] for x in outputs]\n        pred_Y = [preds[0] for preds in batch_scores]\n        final_preds = np.concatenate(pred_Y, axis=1)[0].tolist()\n        true_Y = [preds[1] for preds in batch_scores]\n        final_true = np.concatenate(true_Y, axis=1)[0].tolist()\n        score = classification_report(final_preds, final_true, target_names=classes)\n        print(score)\n        return {'val_loss': epoch_loss.item()}    \n\n    def epoch_end(self, epoch, result):                     # display the losses\n        print(\"Epoch [{}], last_lr: {:.4f}, train_loss: {:.4f}, val_loss: {:.4f}\".format(epoch, result['lrs'][-1], result['train_loss'], result['val_loss']))","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:29.907011Z","iopub.execute_input":"2022-09-11T17:44:29.907424Z","iopub.status.idle":"2022-09-11T17:44:29.924719Z","shell.execute_reply.started":"2022-09-11T17:44:29.907376Z","shell.execute_reply":"2022-09-11T17:44:29.923836Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Вспомогательные функции и класс для размещения необходимого на GPU","metadata":{}},{"cell_type":"code","source":"#helper functions to load the data and model onto GPU\ndef get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu') \n\n\ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():  \n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device       \n\n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)\n\ndevice = get_default_device()\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:29.926370Z","iopub.execute_input":"2022-09-11T17:44:29.926956Z","iopub.status.idle":"2022-09-11T17:44:30.020174Z","shell.execute_reply.started":"2022-09-11T17:44:29.926895Z","shell.execute_reply":"2022-09-11T17:44:30.019339Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#loading training and validation data onto GPU\ntrain_dl = DeviceDataLoader(train_loader, device)\nval_dl = DeviceDataLoader(val_loader, device)","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:30.022269Z","iopub.execute_input":"2022-09-11T17:44:30.022920Z","iopub.status.idle":"2022-09-11T17:44:30.030448Z","shell.execute_reply.started":"2022-09-11T17:44:30.022882Z","shell.execute_reply":"2022-09-11T17:44:30.029600Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Вспомогательные метода для обучения \n* `evaluate` - функция с шагом валидации модели\n* `fit_one_cycle` - функция с циклом обучения модели (включая подсчет валидационных значений)","metadata":{}},{"cell_type":"code","source":"from tqdm.notebook import tqdm\n\n@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n\n    # Set up custom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader)) #schedule the learning rate with OneCycleLR\n\n    for epoch in range(epochs):\n        # Training Phase\n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n\n            # Gradient clipping\n            if grad_clip:\n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:30.032244Z","iopub.execute_input":"2022-09-11T17:44:30.032581Z","iopub.status.idle":"2022-09-11T17:44:30.045446Z","shell.execute_reply.started":"2022-09-11T17:44:30.032547Z","shell.execute_reply":"2022-09-11T17:44:30.044384Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Кастомные классы моделей ","metadata":{}},{"cell_type":"code","source":"from torch import nn\nimport torchvision as vision\n\nclass ResNet18(MultilabelImageClassificationBase):\n    def __init__(self, in_channels, num_classes=10):\n        super().__init__()\n        self.backbone = vision.models.resnet18(pretrained=True)\n        classifier_name, old_classifier = self.backbone._modules.popitem()\n        if isinstance(old_classifier, nn.Sequential):\n            input_shape = old_classifier[-1].in_features\n            old_classifier[-1] = nn.Linear(input_shape,num_classes)\n\n        elif isinstance(old_classifier, nn.Linear):\n            input_shape = old_classifier.in_features\n            old_classifier = nn.Linear(input_shape,num_classes)\n        else:\n            raise Exception(\"Uknown type of classifier {}\".format(type(old_classifier)))\n        self.backbone.add_module(classifier_name, old_classifier)\n\n    def forward(self, X):\n        out = self.backbone(X)\n        out = F.sigmoid(out)\n        return out\n\nclass MobileNetV2(MultilabelImageClassificationBase):\n    def __init__(self, in_channels, num_classes=10):\n        super().__init__()\n        self.backbone = vision.models.mobilenet_v2(pretrained=True)\n        classifier_name, old_classifier = self.backbone._modules.popitem()\n        if isinstance(old_classifier, nn.Sequential):\n            input_shape = old_classifier[-1].in_features\n            old_classifier[-1] = nn.Linear(input_shape,num_classes)\n\n        elif isinstance(old_classifier, nn.Linear):\n            input_shape = old_classifier.in_features\n            old_classifier = nn.Linear(input_shape,num_classes)\n        else:\n            raise Exception(\"Uknown type of classifier {}\".format(type(old_classifier)))\n        self.backbone.add_module(classifier_name, old_classifier)\n\n    def forward(self, X):\n        out = self.backbone(X)\n        out = F.sigmoid(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:30.046812Z","iopub.execute_input":"2022-09-11T17:44:30.047526Z","iopub.status.idle":"2022-09-11T17:44:30.070371Z","shell.execute_reply.started":"2022-09-11T17:44:30.047487Z","shell.execute_reply":"2022-09-11T17:44:30.069321Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model = to_device(MobileNetV2(3, len(classes)), device) #input size: 3, output size: 11, loading model onto GPU","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:30.072899Z","iopub.execute_input":"2022-09-11T17:44:30.073867Z","iopub.status.idle":"2022-09-11T17:44:37.198972Z","shell.execute_reply.started":"2022-09-11T17:44:30.073804Z","shell.execute_reply":"2022-09-11T17:44:37.196230Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Проверка результатов коробочной (еще не обученной) модели","metadata":{}},{"cell_type":"code","source":"history = [evaluate(model, val_dl)]\nhistory","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:37.206746Z","iopub.execute_input":"2022-09-11T17:44:37.209919Z","iopub.status.idle":"2022-09-11T17:44:45.213349Z","shell.execute_reply.started":"2022-09-11T17:44:37.209877Z","shell.execute_reply":"2022-09-11T17:44:45.212660Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Параметры для обучения","metadata":{}},{"cell_type":"code","source":"epochs = 35\nmax_lr = 0.0001\ngrad_clip = 0.1\nweight_decay = 1e-3\nopt_func = torch.optim.Adam","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:45.214691Z","iopub.execute_input":"2022-09-11T17:44:45.215057Z","iopub.status.idle":"2022-09-11T17:44:45.219859Z","shell.execute_reply.started":"2022-09-11T17:44:45.215018Z","shell.execute_reply":"2022-09-11T17:44:45.218799Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"%%time\nhistory += fit_one_cycle(epochs, max_lr, model, train_dl, val_dl,\n                         grad_clip=grad_clip,\n                         weight_decay=weight_decay,\n                         opt_func=opt_func)","metadata":{"execution":{"iopub.status.busy":"2022-09-11T17:44:45.221326Z","iopub.execute_input":"2022-09-11T17:44:45.221721Z","iopub.status.idle":"2022-09-11T18:03:33.962756Z","shell.execute_reply.started":"2022-09-11T17:44:45.221685Z","shell.execute_reply":"2022-09-11T18:03:33.961831Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Сохранение модели\n- В обычном .pth формате \n- В формате torchscript (конвертация модели)","metadata":{}},{"cell_type":"code","source":"model.to(\"cpu\")\ntorch.save(model.state_dict(), \"classif_model.pth\")\nnet = torch.jit.script(model)\nnet.save(\"jit_model.pt\")\nmodel_jit = torch.load(\"jit_model.pt\")","metadata":{"execution":{"iopub.status.busy":"2022-09-11T18:03:33.964438Z","iopub.execute_input":"2022-09-11T18:03:33.964791Z","iopub.status.idle":"2022-09-11T18:03:34.893943Z","shell.execute_reply.started":"2022-09-11T18:03:33.964755Z","shell.execute_reply":"2022-09-11T18:03:34.893018Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Отображение результатов обучения в виде графиков","metadata":{}},{"cell_type":"markdown","source":"### Отображение Loss-а","metadata":{}},{"cell_type":"code","source":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');\n    \nplot_losses(history)","metadata":{"execution":{"iopub.status.busy":"2022-09-11T18:03:34.895272Z","iopub.execute_input":"2022-09-11T18:03:34.895609Z","iopub.status.idle":"2022-09-11T18:03:35.054717Z","shell.execute_reply.started":"2022-09-11T18:03:34.895573Z","shell.execute_reply":"2022-09-11T18:03:35.051960Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Отображение learning rate","metadata":{}},{"cell_type":"code","source":"def plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');\nplot_lrs(history)","metadata":{"execution":{"iopub.status.busy":"2022-09-11T18:03:35.056649Z","iopub.execute_input":"2022-09-11T18:03:35.057074Z","iopub.status.idle":"2022-09-11T18:03:35.186835Z","shell.execute_reply.started":"2022-09-11T18:03:35.057038Z","shell.execute_reply":"2022-09-11T18:03:35.186101Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Ручная проверка результатов обученной модели ","metadata":{}},{"cell_type":"code","source":"def predict_single(image):\n    xb = image.unsqueeze(0)\n    preds = model_jit(xb)\n    prediction = preds[0]\n    show_example(image, prediction)\npredict_single(val_dataset[20][0]) #checking out the predictions of some images from the validation dataset.","metadata":{"execution":{"iopub.status.busy":"2022-09-11T18:03:35.188172Z","iopub.execute_input":"2022-09-11T18:03:35.188490Z","iopub.status.idle":"2022-09-11T18:03:35.841241Z","shell.execute_reply.started":"2022-09-11T18:03:35.188455Z","shell.execute_reply":"2022-09-11T18:03:35.840365Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"predict_single(val_dataset[45][0])","metadata":{"execution":{"iopub.status.busy":"2022-09-11T18:03:35.842958Z","iopub.execute_input":"2022-09-11T18:03:35.843327Z","iopub.status.idle":"2022-09-11T18:03:36.724898Z","shell.execute_reply.started":"2022-09-11T18:03:35.843290Z","shell.execute_reply":"2022-09-11T18:03:36.724023Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"predict_single(val_dataset[100][0])","metadata":{"execution":{"iopub.status.busy":"2022-09-11T18:03:36.726254Z","iopub.execute_input":"2022-09-11T18:03:36.726611Z","iopub.status.idle":"2022-09-11T18:03:36.941974Z","shell.execute_reply.started":"2022-09-11T18:03:36.726572Z","shell.execute_reply":"2022-09-11T18:03:36.941101Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"predict_single(val_dataset[70][0])","metadata":{"execution":{"iopub.status.busy":"2022-09-11T18:03:36.943295Z","iopub.execute_input":"2022-09-11T18:03:36.943667Z","iopub.status.idle":"2022-09-11T18:03:37.161199Z","shell.execute_reply.started":"2022-09-11T18:03:36.943627Z","shell.execute_reply":"2022-09-11T18:03:37.160207Z"},"trusted":true},"execution_count":30,"outputs":[]}]}